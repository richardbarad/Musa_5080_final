---
title: "MUSA_Final"
author: "Richard Barad"
date: "2023-12-04"
output: html_document
---

```{r clear_environment, include=FALSE}

# Lists All Of The Objects In The Work Space
rm(list=ls())
```

```{r knitting_options, include = FALSE}
# Global Options For Knitting Chunks
knitr::opts_chunk$set(echo = T, messages = F, warning = F, error = F)
```

```{r import_libraries, include = FALSE}
library(tidyverse) # Data Science Workflow & Representation
library(tidycensus) # Load United States Census Boundary & Attribute Data
library(sf) # Standardized Way To Encode Spatial Vector Data
library(spdep) # Spatial Dependence: Weighting Schemes & Statistics
library(caret) # Classification and Regression Training
library(FNN) # Fast Nearest Neighbor Search Algorithms & Applications
library(RSocrata) # Download or Upload 'Socrata' Data Sets/Open Data Portals
library(viridis) # Colorblind-Friendly Color Maps Package
library(RColorBrewer) #Package for custom color schemes
library(gridExtra) # Miscellaneous Functions for "Grid" Graphics
library(knitr) # General-Purpose Package for Dynamic Report Generation
library(kableExtra) # Construct Complex Table with 'Kable' and Pipe Syntax
library(plotROC) # Generate Useful ROC Curve Charts for Print and Interactive Use
library(pROC) # Display and Analyze Receiver Operating Characteristic Curves
library(lubridate) # Functions to Work with Date-Times and Time-Spans
library(matchmaker) #Use a dictionary to match and replace values
library(corrr) #For correlation Matrix

source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
```

# Context

The City of Chicago is the third most populated metropolitan area in the United States, boasting a robust culinary landscape with over 15,000 restaurants. Each of the city’s restaurants are subject to a recurring inspection, conducted under the purview of the Department of Public Health’s Division of Food Protection. The task of inspection falls on only 36 accredited sanitarians – leaving each with the responsibility of inspecting around 470 of the 15,000. This resource disparity presents a notable challenge to efficient and effective food safety monitoring.

Addressing resource disparity, Chicago’s solution includes leveraging data and partnering the city's Department of Health with the Advanced Analytics Team housed in the Department of Innovation and Technology. Together, they developed a predictive model aimed at identifying restaurants most likely to have critical food safety violations. The existing model combines historical data, encompassing details from 100,000 past sanitation inspections – along with business characteristics and 311 complaints. The model enables inspectors to prioritize their inspections, concentrating on areas of the greatest risk to fail future food inspections.

The model’s implementation has yielded positive results harnessed by the city of Chicago. Food establishments with critical violations are now more likely to be identified earlier, leading to faster interventions and ultimately creating a increased quality along with availability of dining for Chicago residents. Adopting a more “science-based” initiative highlights the potential of data analytics to enhance public health and economic outcomes, particularly in city governments with limited resources.

# Motivation

Recently, the City of Chicago launched a program through its Department of Public Health and Office of Food Protection, allocating money to a Local Restaurant Improvement Fund. The city aims to appropriate funding resources to establishments facing elevated probability of non-compliance with food safety standards set by the Food Protection Division. As mentioned, on a yearly basis, all retail food establishments throughout Chicago are subject to recurring inspection – aimed at ensuring the quality and safety of food intended for public consumption.

Building on the city's previous work in predictive modeling, we have developed a supplementary forecasting tool tailored to predict a restaurant's likelihood of failing food inspections. This undertaking represents a departure from the conventional procedures common throughout the nation, adopting the more "science-based" methodology employed by the city. However, unlike the city's current use of their predictive model, our model looks to specifically inform the deployment of funds contingent on the forecast of health inspections - harnessing predictive analytics to identify restaurants with an increased susceptibility to regulatory infractions. Our objective is clear: to optimize the impact of the allocated financial resources by strategically directing them towards establishments deemed most at risk of failing food inspections.

The selected restaurants are set to receive financial support, and profit not only monetarily but benefit by engaging in an educational discourse on the importance of passing food inspections. The eligibility for funding hinges on adhering to food quality standards year after year, reinforcing the commitment to maintaining the restaurant's quality. If, however, the establishment falls short of the city's benchmarks, they face the halting of financial support of the city - a harsh reminder of the health standards set in place. Our proposed initiative, thus, unfolds as a structured and strategic intervention, positioning itself as a proactive mechanism for fostering restaurant quality and elevating the overall quality of restaurant offerings throughout the city of Chicago.

# Application

To support the coordinated efforts regarding the restaurant improvement program informed by the predictive modeling, we propose a web-based application. The key features we envision include a heat map, a list of restaurant predictors along with corresponding information, a probability or confidence level toggle section, and supporting search and export features. The web-based application would also be translated into a smartphone format – one that may be more translatable to additional stakeholders outside of the city's decision-makers relating to the Local Restaurant Improvement Fund. The smartphone format would contain the same elements as the browser-based application and will provide a clear direction concerning the funding program. 

Upon thoroughly validating and testing our predictive model for desired outcomes, we intend to proceed with the implementation stage . This stage would involve operationalizing the web-based application with support from a software developer for a seamless and scalable launch. We will first conduct a preliminary beta test with the city of Chicago to gather real-world feedback, and then prepare a full rollout strategy after initial the testing is complete. Developing an effective rollout strategy is crucial for the app's deployment, and we remain committed to continually refine the application based on valuable insights gathered through current inspection data used in our predictve forecasting model.

# Import Data

## Import Inspection Data 

We begin building our predictive model by importing food inspection data from the City of Chicago, covering all records since our analysis focuses on historical failures. The provided information originates from assessments conducted on restaurants and food establishments in Chicago from January 1, 2010 to present. These inspections are carried out by personnel from the Chicago Department of Public Health’s Food Protection Program. Subsequently, the results are entered into a database for approval by a State of Illinois Licensed Environmental Health Practitioner. After importing the data on food inspections, we clean and stardardize the business names along with the facility types. Our selectiveness ensures the predictive model will be built on the types of restaurants/facilities we are trying to target for the Local Restaurant Improvement Fund.

```{r read_data, cache=TRUE}

#Read data from city of Chicago on inspections - we need all data since we look at historical failures

data <- read.socrata("https://data.cityofchicago.org/Health-Human-Services/Food-Inspections/4ijn-s7e5") %>%
  na.omit() %>%
  mutate(inspection_date = ymd(inspection_date),
         year = year(inspection_date),
         month = month(inspection_date),
         address = str_squish(address),
         facility_type = str_squish(str_to_title(facility_type)))

#Clean up a bunch of messy facility names to be able to identify just facilities of (i.e: restaurants, cafes,and bakeries)

data$facility_type[grepl("Restaurant", data$facility_type)] <- "Restaurant"
data$facility_type[grepl("Bakery", data$facility_type)] <- "Bakery"
data$facility_type[grepl("Coffee", data$facility_type)] <- "Coffee Shop"
data$facility_type[grepl("Ice Cream", data$facility_type)] <- "Ice Cream Shop"
data$facility_type[grepl("Deli", data$facility_type)] <- "Deli"
data$facility_type[grepl("Taqueria", data$facility_type)] <- "Restaurant"
data$facility_type[grepl("Hot Dog Station", data$facility_type)] <- "Restaurant"
data$facility_type[grepl("Juice and Salad Bar", data$facility_type)] <- "Restaurant"

#Standardize some names of businesses

data$dba_name[grepl("SEE THRU CHINESE", data$dba_name)] <- 'SEE THRU CHINESE RESTAURANT'
data$dba_name[grepl("JAMAICAN  GATES", data$dba_name)] <- 'JAMAICAN GATES'
```

We download business licenses from the Chicago Data Portal. These licenses are issued by the Department of Business Affairs and Consumer Protection in the City of Chicago, covering the period from 2002 to the present. Next, we proceed with importing and cleaning the data on business licenses. The data retrieval encompasses all licenses issued since 2010, the year our analysis starts given the limitation that our  inspection data does not extend beyond the last 10 years.

```{r download_liscenses, cache=TRUE}

# Download Business Licenses From the Chicago Data Portal

liscenses <- read.socrata("https://data.cityofchicago.org/resource/r5kz-chrr.json?$where=license_start_date%20between%20%272010-01-01T12:00:00%27%20and%20%272021-12-31T14:00:00%27")

```

```{r clean_data}

# Manual Cleaning Of Addresses

corrections <- data.frame(
  bad = c("4623-4627 N BROADWAY  1 & 2","100 E WALTON ST 1 104","436-440 E 79TH ST","1733 W 87TH ST 1ST FLOOR","163 E WALTON ST 2ND F","5640 S UNIVERSITY AVE","5255 W MADISON ST 1 B","111 E 51ST ST 1ST`"),
  good = c("4623-4627 N BROADWAY", "100 E WALTON ST","436 - 440 E 79TH ST","1733 W 87TH ST","163 E WALTON ST","5640 S UNIVERSITY","5255 W MADISON ST","111 E 51ST ST 1ST"),
  stringsAsFactors = FALSE
)

liscenses$address <- match_vec(x=liscenses$address,corrections,from=1,to=2, quiet=TRUE)

liscenses$address[grepl("47 W POLK ST", liscenses$address)] <- "47 W POLK ST"

liscenses$address <- gsub("\\s*\\d+$", "", liscenses$address) #Remove any trailing numbers

liscenses$address <- gsub("\\s*\\d+(ST)?$", "", liscenses$address) #Remove any trailing numbers which are followed by ST

liscenses$address <- gsub("\\s*\\d+(st)?$", "", liscenses$address) #Remove any trailing numbers which are followed by st

#Standardize some business names

liscenses$doing_business_as_name[grepl("SEE THRU CHINESE", liscenses$doing_business_as_name)] <- 'SEE THRU CHINESE RESTAURANT'
liscenses$doing_business_as_name[grepl("THE NEW VALOIS REST", liscenses$doing_business_as_name)] <- 'THE NEW VALOIS REST INC'
liscenses$doing_business_as_name[grepl("CHICAGO MARRIOTT DOWNTOWN", liscenses$doing_business_as_name)] <- 'CHICAGO DOWNTOWN MARRIOTT'
liscenses$doing_business_as_name[grepl("STAR OF SIAM", liscenses$doing_business_as_name)] <- 'STAR OF SIAM'
liscenses$doing_business_as_name[grepl("EL CHILE RESTAURANT & PANCAKE HOUSE.", liscenses$doing_business_as_name)] <- 'EL CHILE RESTAURANT & PANCAKE HOUSE'
liscenses$doing_business_as_name[grepl("FRANCES' DELI & BRUNCHERY", liscenses$doing_business_as_name)] <- "FRANCES' REST & BRUNCHERY"

```

## Read Boundary Datasets

We collect the spatial data for the neighborhoods and the overall city boundary of Chicago, preparing for our spatial analysis and upcoming visualizations. The boundaries of both the city and its neighborhoods were collected from the Chicago Data Portal. The neighborhoods shapefile is refined to include only the neighborhoods and geometry columns, while the city boundary is transformed to a common spatial reference system for compatibility.

```{r neighboorhoods_data}

neighboorhoods <- st_read('Data/neighboorhoods.shp')%>%
  st_transform('ESRI:102271') %>%
  dplyr::select(pri_neigh)

chicagoboundary <- 
  st_read("https://data.cityofchicago.org/api/geospatial/ewy2-6yfk?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271')

```
## Census Data

Wrangle and download census data on percent white population, percent poverty, and median rent by census block.

Income in the Past 12 Months Below Poverty Level: Households are classified as poor when the total income of the householder's family in the last 12 months is below the appropriate poverty threshold.


```{r census_data, results = 'hide'}
variables = c("B17017_002", #Total Households w/ income below poverty line
              "B17017_001", #Total Households 
              "B02001_001", #Total Population
               "B02001_002", #Total White Population
               "B25058_001") #Median Rent

census_data <- get_acs("block group",
        year=2021,
        output='wide',
        geometry=T,
        variables = variables,
        state = 'IL',
        county = 'Cook'
        ) %>%
  st_transform('ESRI:102271') %>%
  select(ends_with('E'),'GEOID') %>%
  rename(poverty = "B17017_001E",
         below_poverty_line = "B17017_002E",
         Total_Population = "B02001_001E",
         White_Population = "B02001_002E",
         Median_Rent = "B25058_001E") %>%
  mutate(Median_Rent = ifelse(is.na(Median_Rent),median(Median_Rent,na.rm=TRUE),Median_Rent),
         pct_non_white = 100 - ifelse(Total_Population == 0,0,White_Population / Total_Population * 100),
         pct_poverty = ifelse(poverty == 0,0, below_poverty_line / poverty * 100)) %>%
  select('Median_Rent','pct_non_white','pct_poverty','GEOID')
```

# Build Training Dataset

We build a training dataset involving filtering and cleaning the inspection data; joining neighborhoods, selecting restaurant types, and specifying a specific year - 2021. We also exclude the limited inspections conducted in the boundary housing the Chicago O'Hare International Airport. We then treat instances labeled as "Pass w/ Conditions" as equivalent to a regular "Pass," and conduct a spatial join with neighborhood data. The resulting cleaned dataset will be used to construct training datasets for the years 2021 and 2020, facilitating our following predictive modeling.

```{r training_data}

filter <- c("Restaurant","Bakery","Tavern","Ice Cream Shop","Deli","Cafe","Coffee Shop","")

#Function to clean inspection data, join to neighboorhoods and filter to just the year of interest
clean_inspections <- function(df,y){
  clean_df <- df %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102271') %>%
  dplyr::filter(year == y & results != 'Out of Business' & results != 'No Entry' & results != 'Not Ready' & results != 'Business Not Located') %>%
  dplyr::filter(facility_type %in% filter) %>%
  mutate(results = ifelse(results=="Pass w/ Conditions","Pass",results),
         fail = ifelse(results=="Fail",1,0)) %>%
  st_join(.,neighboorhoods,predicate = st_intersects) %>%
  mutate(pri_neigh = ifelse(location == "(42.008536400868735, -87.91442843927047)","O'Hare",pri_neigh),
         pri_neigh = ifelse(location %in% c("(41.892249163400116, -87.60951804879336)","(41.89233780863412, -87.6040447589981)"),"Streeterville",pri_neigh)) %>%
  dplyr::filter(pri_neigh != "O'Hare") %>% # Remove Ohare Restaurants
  dplyr::filter(address != '5700 S CICERO AVE') # Remove Midway Restaurants
  return(clean_df)
}

data_2021 <- clean_inspections(data,2021)

data_2020 <- clean_inspections(data,2020)

```


## Determine Age of Restaurant

This code calculates the age of each business by referencing the date of its initial business license application. Specifically relating to retail establishments, it identifies the date when the license was first obtained. The minimum date is then joined back to the original license data and a subset of relevant columns is selected. The resulting age is estimated in days with a maximum age expected to be around 365 days. To enhance accuracy, our code attempts various join methods, as direct joining on the license number did not consistently yield matches. The time frame considered spans 11 years, reflecting the available license data from 2010 onward. This ensures that the analysis focuses on the relevant period up to the year 2021.

``` {r determine_age}
#For each for retail establishment determine date when a license was first obtained
liscense_min <- liscenses %>% group_by(doing_business_as_name,address) %>% summarize(min_date = min(license_start_date)) %>%
  arrange(min_date)

#Join min date back to original licence data and select subset of columns 

liscenses_final <- left_join(liscenses, liscense_min , by = c('doing_business_as_name','address')) %>%
  select(license_id, account_number,legal_name,doing_business_as_name,address,site_number,min_date) %>%
  mutate(license_id = as.integer(license_id))

#Join date business applied for first licence to 2021 inspection data - try to joins a variety of different ways since joining on licence number did not match allways

data_2021_2 <- left_join(data_2021,liscenses_final %>% select(license_id,min_date),by=join_by(license_==license_id)) %>%
  rename(min_date1 = 'min_date') %>%
  left_join(.,liscense_min %>% select(doing_business_as_name,address,min_date),by=join_by(dba_name==doing_business_as_name,address==address),multiple='first') %>%
  rename(min_date2 = 'min_date') %>%
  left_join(.,liscense_min %>% select(doing_business_as_name,address,min_date),by=join_by(aka_name==doing_business_as_name,address==address),multiple='first') %>%
  rename(min_date3 = 'min_date') %>%
  mutate(min_date = pmin(min_date1, min_date2, min_date3, na.rm = TRUE), #Select lowest date where multiple joins worked
         age = as.integer(difftime(inspection_date, min_date, units = 'days')))

data_2021_2 <- data_2021_2 %>%
  mutate(age = ifelse(age<0,0,age),
         age = ifelse(is.na(age),median(age,na.rm=TRUE),age))
  
```
## Estimate Number of previous violations and join to training data

This code estimates the number of previous violations for each business within the dataset. It begins by filtering the data based on the condition that the year is earlier than 2021 and the outcome is defined as 'Fail'. We then group the dataset by business name and address, and the violations are counted. The results are then joined to the training data after being renamed. The purpose of this code is to provide an overview of the historical violation records associated with each business, helping us understand the full compliance history throughout the city of Chicago.

```{r estimate_failures}

fails <- data %>%
  st_drop_geometry() %>%
  dplyr::filter(year<2021 & results == 'Fail') %>%
  group_by(dba_name,address) %>% tally() %>%
  ungroup() %>%
  rename(prev_fails = 'n')

data_2021_2 <- left_join(data_2021_2,fails,by=join_by(dba_name==dba_name,address==address)) %>%
  mutate(prev_fails = replace_na(prev_fails,0),
         fails_Per_year = replace_na(prev_fails/(age/365),0),
         fails_Per_year =  ifelse(is.infinite(fails_Per_year), prev_fails, fails_Per_year))

```

## Join Census Data to training dataset

Here, we integrate census data into the training dataset through a spatial join, associating each restaurant record with corresponding census tract information. A calculation is performed on neighborhood-level statistics, including mean values for variables such as percent poverty, median rent, and percent non-white - organized by the neighborhoods. We handle cases where a restaurant lacks a specific census tract association by performing a left join, replacing missing values in the joined columns with the mean neighborhood statistics. In the end, each restaurant entry is supported with the census socioeconomic information, producing a more detailed analysis of its context within the surrounding demographic landscape of Chicago.

```{r join_censu_data}
data_2021_2 <- data_2021_2 %>%
  st_join(.,census_data,predicate = st_intersects)

means_neigh <- data_2021_2 %>% 
  st_drop_geometry() %>% 
  group_by(pri_neigh) %>% summarize_at(vars("pct_poverty","Median_Rent","pct_non_white"),mean,na.rm=TRUE)

data_2021_2 <- left_join(data_2021_2,means_neigh,by='pri_neigh') %>%
  mutate(pct_poverty = ifelse(is.na(pct_poverty.x),pct_poverty.y,pct_poverty.x),
         Median_Rent = ifelse(is.na(Median_Rent.x),Median_Rent.y,Median_Rent.x),
         pct_non_white = ifelse(is.na(pct_non_white.x),pct_non_white.y,pct_non_white.x)) %>%
  select(-ends_with('.x'),-ends_with('.y'))

```

## Exploratory Analysis

```{r guick_map1, include = TRUE, fig.keep = "none"}


custom_colors <- c("Pass" = "#41B6E6", "Fail" = "#E4002B")

ggplot() +
  geom_sf(data = neighboorhoods) +
  geom_sf(data = data_2021, aes(color = results), size = 0.5) +
  scale_color_manual(values = custom_colors) +
  theme_void()

```
This map illustrates the outcomes of food inspections in 2021, which will serve as the training data for our model. Notably, a concentration of inspection points is observed in and around the Loop, as well as along traffic corridors. The spatial distribution reveals clear clustering in areas where inspection failures are prevalent.

![](Food Inspections PF_POINT.jpg)

```{r guick_map2, include = TRUE, fig.keep = "none"}
neigh_summ <- data_2021 %>% st_drop_geometry() %>%
  group_by(pri_neigh,results) %>% tally() %>%
  ungroup() %>%
  spread(key=results,value=n) %>%
  mutate(Fail = replace_na(Fail,0),
    pct_pass = Pass / (Fail + Pass) * 100,
    pct_fail = Fail / (Fail + Pass) * 100,
    quartile = ntile(pct_fail, 4),
    decile = ntile(pct_fail, 10)) %>%
  left_join(neighboorhoods,.,by='pri_neigh')
  
ggplot() +
  geom_sf(data = neigh_summ, aes(fill = pct_pass)) +
  scale_fill_distiller(palette = "Blues") +
  theme_void()
```
This map depicts the percent of restaurants that pass inspections. Restaurants in Garfield Park, New City and Chinatown have the highest fail rates in the city. 

![](Food Inspections PP_CHOROPLETH.jpg)

```{r guick_chart, fig.width=14}

neigh_summ %>% 
  st_drop_geometry() %>%
  select(pri_neigh,pct_pass,pct_fail) %>%
  gather(pass,rate,-pri_neigh) %>%
  na.omit() %>%
  ggplot(aes(x=pri_neigh,y=rate,fill=pass))+
  geom_bar(position='stack',stat='identity')+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

In the figure above, we have display a stacked bar plot to illustrate the pass and fail rates across various neighborhoods throughout Chicago. The x-axis denotes different neighborhoods, the y-axis represents rates, and the bars are stacked to visually depict the distribution of pass and fail rates of food inspections. Notably, Riverdale, Millennium Park, and Jackson Park emerge as the neighborhoods with the highest percentage of pass rates in Chicago, while Garfield Park, Navy City, and Chinatown exhibit the largest percentage of fail rates for food inspections. Note: The pass/fail rates may not be completely representative since they can be influenced by the number of restaurants, or the lack thereof in Chicago's neighborhoods.

# Knearest Neighboor Analysis

## 311 Datasets

```{r get_lots_311, cache=TRUE}

sanitation <- read.socrata("https://data.cityofchicago.org/resource/v6vf-nfxy.json?SR_SHORT_CODE='SCB'")
rodents <- read.socrata("https://data.cityofchicago.org/resource/v6vf-nfxy.json?SR_SHORT_CODE='SGA'")

clean_311 <- function(df){
  clean_df <- df %>%
  mutate(created_date = ymd_hms(created_date),
         year = year(created_date)) %>%
  dplyr::filter(year == 2020 & duplicate=='FALSE') %>%
  dplyr::filter(!is.na(longitude)) %>%
  dplyr::filter(!is.na(latitude)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102271')
  return(clean_df)
}

sanitation2 <- clean_311(sanitation)
rodents2 <- clean_311(rodents)
```
```{r complaints_visualized, fig.width=9.5, fig.height=4}

grid.arrange(
  ncol = 2,
  ggplot() + 
    geom_sf(data = chicagoboundary) +
    geom_sf(data = sanitation2, colour = "#41B6E6", size = 0.1, show.legend = "point") +
    labs(title = "Sanitation Code Complaints"), 

  ggplot() + 
  geom_sf(data = chicagoboundary, fill = "#FFFFFF") +
  stat_density2d(data = data.frame(st_coordinates(sanitation2)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 linewidth = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_distiller(palette = "RdBu") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Sanitation Code Complaints") +
  theme(legend.position = "none"))
```

```{r baiting_visualized, fig.width=9.5, fig.height=4}
grid.arrange(ncol=2,
ggplot() + 
  geom_sf(data = chicagoboundary) +
  geom_sf(data = rodents2, colour="#41B6E6", size=0.1, show.legend = "point") +
  labs(title= "Rodent Baiting")+
  theme_void(),

ggplot() + 
  geom_sf(data = chicagoboundary, fill = "#FFFFFF") +
  stat_density2d(data = data.frame(st_coordinates(rodents2)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 linewidth = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_distiller(palette = "RdBu") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Rodent Baiting") + 
  theme_void()+
  theme(legend.position = "none"))
```

## Theft Data

``` {r read_crime}
thefts <- read.socrata('https://data.cityofchicago.org/resource/qzdf-xmn8.json?primary_type=THEFT') %>%
  mutate(created_date = ymd_hms(date),
         year = year(date)) %>%
  dplyr::filter(!is.na(longitude)) %>%
  dplyr::filter(!is.na(latitude)) %>%
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102271')
```

```{r thefts, fig.width=9.5, fig.height=4}
grid.arrange(ncol=2,
ggplot() + 
  geom_sf(data = chicagoboundary) +
  geom_sf(data = thefts, colour="#41B6E6", size=0.1, show.legend = "point") +
  labs(title= "Theft Events"),

ggplot() + 
  geom_sf(data = chicagoboundary, fill = "#FFFFFF") +
  stat_density2d(data = data.frame(st_coordinates(thefts)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 linewidth = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_distiller(palette = "RdBu") +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Theft Events") + 
  theme(legend.position = "none"))
```

```{r, nearest_neighboor_analysis, fig.width=9.5, fig.height=4}
# Producing a Nearest Neighbor Analysis Between the 2021 Inspection Data and 2020 Sanitation Complaints and 2020 Rodent Reports

buffer <- st_buffer(data_2021_2,1000)

rest_1000m <- st_join(buffer,data_2020 %>% dplyr::select('geometry','results')) %>%
  st_drop_geometry() %>%
  group_by(inspection_id,results.y) %>% tally() %>%
  ungroup() %>% spread(key=results.y,value=n) %>%
  select(-'<NA>') %>%
  replace(is.na(.), 0) %>%
  mutate(prev_year_pct_fail = Fail / (Pass + Fail)) %>%
  replace(is.na(.), 0) %>%
  select(inspection_id,prev_year_pct_fail)

data_2021_2 <- data_2021_2 %>%
    mutate(rodents.nn = nn_function(st_coordinates(data_2021_2),st_coordinates(rodents2),k = 50),
           log_rodents.nn = log(rodents.nn),
           sanitation.nn = nn_function(st_coordinates(data_2021_2),st_coordinates(sanitation2),k = 50),
           log_sanitation.nn = log(sanitation.nn),
          thefts.nn = nn_function(st_coordinates(data_2021_2),st_coordinates(thefts),k = 50),
          log_thefts.nn = log(thefts.nn),
          log_pct_poverty = log(1 + pct_poverty)) %>%
  left_join(.,rest_1000m, by = 'inspection_id') %>%
  left_join(.,neigh_summ %>%st_drop_geometry(), by = 'pri_neigh')

ggplot() +
  geom_sf(data = chicagoboundary, fill = "grey80") +
  geom_sf(data = data_2021_2, aes(color = prev_year_pct_fail * 100), size = 0.5) +
  scale_color_viridis_c(name = "Percent Failure")+
  labs(title = "Percent of Restaurants Within 1km That Failed in 2020") +
  theme_void()
```

```{r nn_maps, fig.width=10}
my_breaks = c(50,100, 250, 500, 1000, 2000)

grid.arrange(ncol=3,
  
ggplot() +
  geom_sf(data = chicagoboundary, fill = "grey80") +
  geom_sf(data = data_2021_2, aes(color = rodents.nn), size = 0.5) +
  scale_color_viridis_c(name = "distance", trans = "log",breaks = my_breaks, labels = my_breaks)+
  labs(title = "Rodent Complaint") +
  theme_void(),

ggplot() +
  geom_sf(data = chicagoboundary, fill = "grey80") +
  geom_sf(data = data_2021_2, aes(color = sanitation.nn), size = 0.5) +
  scale_color_viridis_c(name = "distance", trans = "log",breaks = my_breaks, labels = my_breaks)+
  labs(title = "Sanitation Complaint") +
  theme_void(),

ggplot() +
  geom_sf(data = chicagoboundary, fill = "grey80") +
  geom_sf(data = data_2021_2, aes(color = thefts.nn), size = 0.5) +
  scale_color_viridis_c(name = "distance", trans = "log",breaks = my_breaks, labels = my_breaks)+
  labs(title = "Thefts") +
  theme_void()
)
```

# Explore Predictors

Having identified the predictors which we plan to use to predict restaurant failures we can explore the data to determine if there is a difference between the mean value for restaurants that fail inspection and restaurants that pass inspection. Based on the plots below, we can conclude that restaurants that fail tend to have a higher age, a higher number of previous fails per year and a higher number of previous failures. Restaurants that fail also tend to be situated in areas with a lower median rent and higher percent of residents living in poverty. Restaurants that fail also tend to have a lower average distance to the nearest 50 theft event, 311 sanitation complaints, and 311 rodent complaints. 

```{r examine_means,fig.width=10,fig.height=4}

# Look at the means for each continuous predictor

quant_predictors <- c("rodents.nn","sanitation.nn","age","prev_fails","Median_Rent","pct_poverty","fails_Per_year","thefts.nn","prev_year_pct_fail","decile")

data_2021_2_means <- data_2021_2 %>%
  st_drop_geometry() %>%
  group_by(results) %>% summarize_at(vars(quant_predictors),mean,na.rm=TRUE) 

data_2021_2_means %>%
  gather(key='variable',value='mean',-results) %>%
  ggplot(aes(x=results,y=mean,fill=results))+
  geom_bar(stat='identity')+
  facet_wrap(~variable,scales = "free",nrow=2)+
  scale_fill_manual(values=c("#E4002B","#41B6E6"),name='Pass Inspection')+
  labs(x="Pass Inspection", y="Mean Value", 
      title = "Feature associations with likelihood of passing inspection",subtitle="Mean Values")+
  theme_bw()

```

```{r data visualization numeric2, fig.width=14, fig.height=7,echo=FALSE,eval=FALSE}

# This chunk will be deleted in final writeup. I do not think it is needed and is kinda confusing.

data_2021_2 %>% dplyr::select(quant_predictors,'results') %>% st_drop_geometry() %>%
  gather(key='variable',value='value',-results) %>%
  ggplot()+
  geom_density(aes(x=value,color=results))+
  facet_wrap(~variable,scales = "free")+
  scale_color_manual(values=c("#E4002B","#41B6E6"),name='Enter Housing Subsidy')+
  labs(x="Enter Housing Subsidy Program", y="Density", 
      title = "Feature associations with likelihood of entering housing subsidy program",
      subtitle = "Continous outcomes")+
  theme_bw()

```

## Means Table

The table below shows the mean value for each predictor for restaurants that failed inspection an restaurants that passed inspection. The table also includes the results of a series of t-tests. T-tests can be used to determine if the mean values are statistically different from each other, and p-values that are less than 0.05 indicate that the mean value for restaurants that passed inspection and restaurants that failed each other are statistically different from each other. The difference in the mean is statistically significant for all predictors except for the age of the restaurant and the distance to nearest 50 reported theft events.  

```{r table_means}

t_age <- t.test(data_2021_2$age~data_2021_2$fail)$p.value
t_decile <- t.test(data_2021_2$decile~data_2021_2$fail)$p.value
t_prev_fails <- t.test(data_2021_2$prev_fails~data_2021_2$fail)$p.value
t_pct_poverty <- t.test(data_2021_2$pct_poverty~data_2021_2$fail)$p.value
t_Median_Rent <- t.test(data_2021_2$Median_Rent~data_2021_2$fail)$p.value
t_rodents.nn <- t.test(data_2021_2$rodents.nn~data_2021_2$fail)$p.value
t_sanitation.nn <- t.test(data_2021_2$sanitation.nn~data_2021_2$fail)$p.value
t_fails_per_year <- t.test(data_2021_2$fails_Per_year~data_2021_2$fail)$p.value
t_thefts <- t.test(data_2021_2$thefts~data_2021_2$fail)$p.value
t_prev_year <- t.test(data_2021_2$prev_year_pct_fail~data_2021_2$fail)$p.value

p_values = c(t_age,t_decile,t_fails_per_year,t_Median_Rent,t_pct_poverty,t_prev_fails,t_prev_year,t_rodents.nn,t_sanitation.nn,t_thefts)

data_2021_2_means %>%
  gather('variable','value',-results) %>%
  spread('results','value') %>%
  cbind(.,p_values, variables_text=c('Age (days open)','Neighboorhood Decile (1-10)','Fails per year open','Median Rent (USD)','Percent Poverty','Previous Number of Fails','Percent of Restaurants Failing in previous Year within 1k buffer','Average Distance distance to nearest 50 rodent complaints (meters)','Average Distance to nearest 50 sanitation complaints (meters)','Average distance to nearest 50 reported thefts (meters)')) %>%
  dplyr::select(variables_text,Fail,Pass,p_values) %>%
  kbl(col.names=c('Variables','Fail (Mean)','Pass (Mean)','t-test (p-value)')) %>%
  kable_classic_2()
  
```

# Check Correlations

Next, we examine the correlation matrix for the predictors in our model. Checking the correlation is an important step because it allows us to determine if there is any severe multicollinearity between any of predictors. A logistic regression should not include variables that exhibit multicollinearity. We consider variables to be severe multicollinear if the r value is greater than 0.7 or less than -0.7. Based on the correlation matrix below we can conclude that is no severe  multicollinerity between our numeric predictors and they can all be safely included in our predictive model. 

```{r correlation_matrix}

data_2021_2 %>% dplyr::select(quant_predictors) %>%
  st_drop_geometry() %>%
  correlate() %>% 
  autoplot() +
  geom_text(aes(label = round(r,digits=2)),size = 3)

```

# Models

Having identified our potential predictors, explored our data we now proceed to the modelling stage of our analysis. We develop three different models in order to compare the accuracy of models developed using different predictors. 

## Split into Training and Test Data

The dataset containing restaurants inspection results for 2021 is split into two parts. Seventy percent of the data is put into the training dataset which will be used to train the logistic models. Thirty percent of the data is placed in the test dataset. We will predict the inspection result for each of the data points in the test dataset and compare the predicted outcome to the actual outcomes to assess the model accuracy. 

```{r}

# Split Data Into 70:30 Training Dataset and Test Dataset

# Taken From Housing Subsidy Assignment, 6.3.1 In Book

set.seed(3456)
trainIndex <- createDataPartition(y = paste(data_2021_2$pri_neigh, data_2021_2$fail), p = .70,list = FALSE,times = 1)
train <- data_2021_2[ trainIndex,]
test  <- data_2021_2[-trainIndex,]

```

## Build Models

### Model 1

Our first model includes the following predictors: Age of restaurant, previous number of fails, fails per year open, Median Rent of census block, percent poverty of census block, and the theft, rodent, and sanitation nearest neighbor variables. The table below shows the Beta Coefficient and p-value for each of the predictors in the model. Examining the p-value can help provide a sense of the predictive power of the variable. The fails_Per_year variable has a very high p-value indicating that its predictive power is limited. For this reason, we decide to exclude the fails_Per-year variable from other models.  

``` {r make_model1}
model1 <- glm(fail ~ age + prev_fails + Median_Rent + rodents.nn + sanitation.nn + thefts.nn + pct_poverty + fails_Per_year,
                     data = train,
                     family="binomial" (link="logit"))

summary(model1)$coefficients[, c('Estimate','Pr(>|z|)')] %>%
  kbl(col.names = c('Beta Coefficient','p-value')) %>%
  kable_classic()
```

### Model 2

In our second model, we include all the same variable as model one except for the fails_Per_year variable. We also add the 

```{r make_model2}
model2 <- glm(fail ~ age + prev_fails + Median_Rent + rodents.nn + sanitation.nn + thefts.nn + pct_poverty + prev_year_pct_fail,
                     data = train,
                     family="binomial" (link="logit"))

summary(model2)$coefficients[, c('Estimate','Pr(>|z|)')] %>%
  kbl(col.names = c('Beta Coefficient','p-value')) %>%
  kable_classic()

```

model3 <- glm(fail ~ age + prev_fails + Median_Rent + rodents.nn + sanitation.nn + thefts.nn + pct_poverty + prev_year_pct_fail + decile + risk,
                     data = train,
                     family="binomial" (link="logit"))

summary(model1)
summary(model2)
summary(model3)
```


``` {r make_predictiosn}

testProbs <- data.frame(inspection_id = test$inspection_id,
                        Outcome = as.factor(test$fail),
                        probs1 = predict(model1, test, type= "response"),
                        probs2 = predict(model2, test, type= "response"),
                        probs3 = predict(model3, test, type= "response"))
```

```{r roc_curve, warning = FALSE, message = FALSE}
# Need to add anoitation of area under curve
pROC::auc(testProbs$Outcome,testProbs$probs1)
pROC::auc(testProbs$Outcome,testProbs$probs2)
pROC::auc(testProbs$Outcome,testProbs$probs3)

ggplot(testProbs) +
  geom_roc(aes(d = as.numeric(Outcome), m = probs1),n.cuts = 50, labels = FALSE, colour = "orange") +
  geom_roc(aes(d = as.numeric(Outcome), m = probs2),n.cuts = 50, labels = FALSE, colour = "blue") +
  geom_roc(aes(d = as.numeric(Outcome), m = probs3),n.cuts = 50, labels = FALSE, colour = "red") +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1, color = 'grey60') +
  labs(title = "ROC Curve - Housing Subsidy Model")+
  theme_bw()
```

## Model Results

## Density

```{r density_chart, fig.width=10}

ggplot(testProbs, aes(x = probs3, fill = as.factor(Outcome))) + 
  geom_density() +
  facet_wrap(~Outcome,ncol=1,labeller = as_labeller(c('0'='Pass','1'='Fail'))) +
  scale_fill_manual(values=c("#41B6E6","#E4002B"),name="Outcome")+
  scale_x_continuous(limits=c(0,1))+
  labs(x = "Probability of Failure", y = "Density",
       title = "Density of Predicted Probabilities for Restaurants in Test Dataset that failed and passed inspection") +
  theme(strip.text.x = element_text(size = 18),
        legend.position = "none")+
  theme_bw()

```

# Sensitivity and Specificty Table

```{r, sens_spec_table}

tresholds <- c(0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6)
sensitivity <- c()
specificity <- c()
accuracy <- c()
pred_fail <- c()

for (t in tresholds){
  predictions <- as.factor(ifelse(testProbs$probs3 > t,1,0))
  r <- caret::confusionMatrix(testProbs$Outcome,predictions,positive="1")
  sensitivity <- append(sensitivity,r$byClass[1] * 100) 
  specificity <- append(specificity,r$byClass[2] * 100)
  accuracy <- append(accuracy,r$overall[1] * 100)
  pred_fail <- append(pred_fail,sum(r$table[3:4]))
}

cbind(tresholds,sensitivity,specificity,accuracy,pred_fail) %>%
  as_data_frame() %>%
  kbl(col.names = c("Treshold",'Sensitivity (%)','Specificity (%)', 'Accuracy (%)', 'Predicted Failures')) %>%
  kable_classic_2()
```

```{r confusion_matrix}

predictions <- as.factor(ifelse(testProbs$probs3 > 0.3,1,0))
confusematrix <- caret::confusionMatrix(testProbs$Outcome,predictions,positive="1")

confusematrix$table %>%
  data.frame() %>%
  spread(key=Reference,value=Freq) %>%
  rename('Pass' = '0', 'Fail' = '1') %>%
  mutate(Total = Pass + Fail,
         Prediction = c('Pass','Fail')) %>%
  kbl() %>%
  add_header_above(header=c(" " = 1,"Actual" = 3)) %>%
  kable_minimal()

```

# Cross Validation

50 K fold cross validation

```{r cross_validation}

ctrl <- trainControl(method = "cv", number = 50, classProbs = TRUE, savePredictions = TRUE, summaryFunction = twoClassSummary)

cvFit <- train(
  results ~ age + prev_fails + Median_Rent + rodents.nn + sanitation.nn + thefts.nn + risk + pct_poverty + prev_year_pct_fail + decile + risk,
  data = data_2021_2,
  method = "glm",
  family = "binomial",
  metric = "ROC",
  trControl = ctrl
)

#This is doing the cross validation with a custom threshold, using a treshold of 0.5 is not useful and could not figure out how to use a different threshold with the summaryFunction

pred <- cvFit$pred %>%
  mutate(pred = (ifelse(Fail > 0.30,'Fail','Pass')),
        type = case_when(pred == 'Fail' & obs == 'Fail' ~ 'TP',
                         pred == 'Pass' & obs == 'Pass' ~ 'TN',
                         pred == 'Fail' & obs == 'Pass' ~ 'FP',
                         pred == 'Pass' & obs == 'Fail' ~ 'FN')) %>%
  group_by(Resample,type) %>% tally %>%
  spread(key=type,value=n) %>%
  mutate(Sensitvity = TP / (TP + FN),
         Specificity = TN / (FP + TN),
         Accuracy = (TP + TN) / (TP + TN + FP + FN)) %>%
  inner_join(.,cvFit$resample %>% dplyr::select(Resample,ROC),by='Resample') %>%
  dplyr::select('Sensitvity','Specificity','Accuracy','ROC') %>%
  pivot_longer(cols=c('Sensitvity','Specificity','Accuracy','ROC'),names_to='metric',values_to='value') %>%
  group_by(metric) %>%
  mutate(mean = mean(value))

ggplot(data=pred,aes(value)) + 
  geom_histogram(bins=50, fill = '#E4002B')+
  facet_wrap(~metric) +
  geom_vline(aes(xintercept = mean), colour = "#41B6E6", linetype = 3, size = 1.5) +
  scale_x_continuous(limits = c(0, 1)) +
  labs(x="Goodness of Fit", y="Count", title="CV Goodness of Fit Metrics using 0.3 treshold",subtitle = "Across-fold mean reprented as dotted lines")+
  theme_bw()


```

# Looking at Senstivity and Specifity by Neighboorhood - may want to remove (it makes model look really bad :) with 0 specificity in so many neighboorhoods ).

```{r sens_spec_neigh}
testProbs %>%
  select(inspection_id,Outcome,probs3) %>%
  left_join(.,test %>% dplyr::select(pri_neigh,inspection_id),by='inspection_id') %>%
  mutate(predoutcome = ifelse(probs3 > 0.30,1,0),
         type = case_when(Outcome == 0 & predoutcome == 0 ~ 'TN',
                          Outcome == 1 & predoutcome == 1 ~ 'TP',
                          Outcome == 1 & predoutcome == 0 ~ 'FN',
                          Outcome == 0 & predoutcome == 1 ~ 'FP')) %>%
  group_by(pri_neigh,type) %>% tally() %>%
  spread(key=type,value=n) %>%
  replace(is.na(.), 0) %>%
  mutate(Sensitvity = TP / (TP + FN),
         Specificity = TN / (FP + TN),
         Accuracy = (TP + TN) / (TP + TN + FP + FN)) %>%
  select(Sensitvity,Specificity,Accuracy,pri_neigh) %>%
  gather(accuracy_measure,value,-pri_neigh) %>%
  inner_join(neighboorhoods,.,by='pri_neigh')%>%
  ggplot()+
  geom_sf(aes(fill=value * 100))+
  facet_wrap(~accuracy_measure)+
  scale_fill_viridis(option='rocket',name='Percentage')+
  theme_bw()+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank())
```

# Look at Model Performance by Neighboorhood Type (i.e: High, Medium, and Low Failure Rate)

```{r sens_spec_neigh}

testProbs %>%
  select(inspection_id,Outcome,probs3) %>%
  left_join(.,test %>% dplyr::select(quartile,inspection_id),by='inspection_id') %>%
  mutate(predoutcome = ifelse(probs3 > 0.30,1,0),
         type = case_when(Outcome == 0 & predoutcome == 0 ~ 'TN',
                          Outcome == 1 & predoutcome == 1 ~ 'TP',
                          Outcome == 1 & predoutcome == 0 ~ 'FN',
                          Outcome == 0 & predoutcome == 1 ~ 'FP')) %>%
  group_by(quartile,type) %>% tally() %>%
  ungroup() %>%
  spread(key=type,value=n) %>%
  replace(is.na(.), 0) %>%
  mutate(Sensitvity = round((TP / (TP + FN)) * 100,2),
         Specificity = round((TN / (FP + TN)) * 100,2),
         Accuracy = round(((TP + TN) / (TP + TN + FP + FN)) * 100,2),
         Neighboor = c('Very Low (Quartile 1)','Low (Quartile 2)','Medium (Quartile 3)','High (Quartile 4)')) %>%
  select(Neighboor,Sensitvity,Specificity,Accuracy) %>%
  kbl(col.names = c('Historical Neighboorhood Failure Rate','Sensitivity (%)','Specificity (%)','Accuracy (%)')) %>%
  kable_classic_2()
```


# Look at Model Performance by Race

```{r}

testProbs %>%
  select(inspection_id,Outcome,probs3) %>%
  left_join(.,test %>% dplyr::select(pct_non_white,inspection_id),by='inspection_id') %>%
  mutate(predoutcome = ifelse(probs3 > 0.30,1,0),
         majority_pop = ifelse(pct_non_white > 50,'Non-White','White'),
         type = case_when(Outcome == 0 & predoutcome == 0 ~ 'TN',
                          Outcome == 1 & predoutcome == 1 ~ 'TP',
                          Outcome == 1 & predoutcome == 0 ~ 'FN',
                          Outcome == 0 & predoutcome == 1 ~ 'FP')) %>%
  group_by(majority_pop,type) %>% tally() %>%
  ungroup() %>%
  spread(key=type,value=n) %>%
  replace(is.na(.), 0) %>%
  mutate(Sensitvity = round((TP / (TP + FN)) * 100,2),
         Specificity = round((TN / (FP + TN)) * 100,2),
         Accuracy = round(((TP + TN) / (TP + TN + FP + FN)) * 100,2)) %>%
  select(majority_pop,Sensitvity,Specificity,Accuracy) %>%
  kbl(col.names = c('Majority Population','Sensitivity (%)','Specificity (%)','Accuracy (%)')) %>%
  kable_classic_2()

```